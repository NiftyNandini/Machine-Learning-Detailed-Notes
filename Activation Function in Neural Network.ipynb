{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd70a57",
   "metadata": {},
   "source": [
    "# Activation Functions in Machine Learning\n",
    "\n",
    "## What is an Activation Function?\n",
    "\n",
    "In machine learning, especially in neural networks, an **activation function** is a mathematical function used to introduce non-linearity into the model. This helps the model learn complex patterns in the data. Without an activation function, a neural network would only be able to model linear relationships, which limits its capability.\n",
    "\n",
    "Activation functions are applied to the output of neurons (also called nodes) in each layer of the neural network. They determine whether a neuron should be activated or not, based on its input.\n",
    "\n",
    "---\n",
    "\n",
    "## Importance of Activation Functions\n",
    "\n",
    "1. **Non-linearity**: Activation functions introduce non-linear properties to the network, enabling it to learn complex patterns.\n",
    "2. **Control Output**: They control the output of a neuron and help in deciding whether it should activate or not.\n",
    "3. **Enabling Deep Networks**: Without activation functions, a neural network with multiple layers would behave like a single-layer network, limiting its performance.\n",
    "4. **Feature Transformation**: They help in transforming the weighted sum of inputs into a format that is useful for the next layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Activation Functions\n",
    "\n",
    "### 1. **Sigmoid Activation Function**\n",
    "\n",
    "The **Sigmoid** function squashes the input to a value between 0 and 1, making it useful for binary classification problems. It’s a smooth, differentiable function.\n",
    "\n",
    "#### Formula:\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "- **Range**: (0, 1)\n",
    "- **Properties**: \n",
    "  - Differentiable.\n",
    "  - Smooth output.\n",
    "  - The output is never exactly 0 or 1 (it approaches but never reaches).\n",
    "- **Limitations**: \n",
    "  - **Vanishing Gradient**: It suffers from vanishing gradients for very large or very small inputs.\n",
    "  - **Slow convergence** due to gradients becoming very small.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Hyperbolic Tangent (Tanh)**\n",
    "\n",
    "The **Tanh** function is similar to sigmoid but maps the input to a range between -1 and 1, which makes it more centered and can help improve training.\n",
    "\n",
    "#### Formula:\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$\n",
    "\n",
    "- **Range**: (-1, 1)\n",
    "- **Properties**:\n",
    "  - The output is centered around 0.\n",
    "  - More powerful than the sigmoid because it can have both negative and positive values.\n",
    "- **Limitations**:\n",
    "  - **Vanishing Gradient**: Similar to the sigmoid, it suffers from vanishing gradients.\n",
    "  - **Not Zero-Centered**: Though the output is between -1 and 1, it still suffers from issues when the data isn't normalized.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **ReLU (Rectified Linear Unit)**\n",
    "\n",
    "The **ReLU** function is the most widely used activation function, especially for deep networks. It’s computationally efficient and solves the vanishing gradient problem to some extent.\n",
    "\n",
    "#### Formula:\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "- **Range**: [0, ∞)\n",
    "- **Properties**:\n",
    "  - Efficient and computationally simple.\n",
    "  - Doesn’t suffer from vanishing gradients.\n",
    "  - Enables the model to learn faster.\n",
    "- **Limitations**:\n",
    "  - **Dying ReLU Problem**: Neurons can \"die\" and stop learning if the output is always 0 for all inputs (e.g., for negative inputs).\n",
    "  - **Unbounded Output**: Can lead to very large outputs, making optimization unstable.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Leaky ReLU**\n",
    "\n",
    "**Leaky ReLU** is an improved version of ReLU designed to address the \"dying ReLU\" problem by allowing a small, non-zero gradient when the input is negative.\n",
    "\n",
    "#### Formula:\n",
    "$$\n",
    "\\text{Leaky ReLU}(x) = \\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Range**: (-∞, ∞)\n",
    "- **Properties**:\n",
    "  - Allows a small gradient for negative inputs (α is a small constant, typically 0.01).\n",
    "  - Helps in keeping neurons alive even when they should theoretically output 0.\n",
    "- **Limitations**:\n",
    "  - Like ReLU, can have very large outputs, leading to optimization instability.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Softmax Activation Function**\n",
    "\n",
    "The **Softmax** function is used in the output layer of classification problems, especially for multi-class classification. It converts the outputs into probability distributions.\n",
    "\n",
    "#### Formula:\n",
    "$$\n",
    "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$$\n",
    "Where:\n",
    "- \\(z_i\\) is the input for the \\(i^{th}\\) neuron.\n",
    "- The denominator is the sum of the exponentials of all inputs to ensure the output sums to 1.\n",
    "\n",
    "- **Range**: (0, 1) for each output class.\n",
    "- **Properties**:\n",
    "  - Converts logits (raw output values) into probabilities.\n",
    "  - Useful for multi-class classification problems.\n",
    "- **Limitations**:\n",
    "  - Computationally expensive for large outputs.\n",
    "  - Doesn’t work well for non-multi-class problems.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Swish Activation Function**\n",
    "\n",
    "The **Swish** function is a newer activation function proposed by researchers at Google. It is smooth, non-monotonic, and has been shown to outperform ReLU and its variants in some cases.\n",
    "\n",
    "#### Formula:\n",
    "$$\n",
    "\\text{Swish}(x) = x \\cdot \\sigma(x)\n",
    "$$\n",
    "Where \\( \\sigma(x) \\) is the sigmoid function.\n",
    "\n",
    "- **Range**: (-∞, ∞)\n",
    "- **Properties**:\n",
    "  - Smooth and non-monotonic, helps improve model performance in some cases.\n",
    "  - Allows negative outputs, unlike ReLU.\n",
    "- **Limitations**:\n",
    "  - Computationally expensive compared to ReLU.\n",
    "  - Not as widely adopted yet.\n",
    "\n",
    "---\n",
    "\n",
    "## Choosing the Right Activation Function\n",
    "\n",
    "- **Sigmoid**: Good for binary classification but suffers from vanishing gradients and slow convergence.\n",
    "- **Tanh**: Better than sigmoid, but still suffers from vanishing gradients.\n",
    "- **ReLU**: The default choice for most deep learning models, fast and effective.\n",
    "- **Leaky ReLU**: A solution for dying ReLU, suitable for deep networks.\n",
    "- **Softmax**: Best for multi-class classification tasks.\n",
    "- **Swish**: A newer function, potentially better than ReLU in some scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Activation functions are critical in neural networks as they introduce the necessary non-linearity, allowing the model to learn complex patterns in data. While **ReLU** and **Softmax** are commonly used in many modern networks, choosing the right activation function depends on the problem you are trying to solve and the specific behavior you want from your model.\n",
    "\n",
    "By understanding the strengths and weaknesses of each activation function, you can make more informed choices about which to use in your machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114d63c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
